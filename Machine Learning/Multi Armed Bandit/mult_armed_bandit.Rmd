---
title: "Multi Armed Bandit - Is it better than A/B testing?"
author: "Riki Saito"
date: "May 28, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction

In the domain of webpage optimization, A/B testing is the most commonly and widely used experimentation method to compare two (or more) versions of a webpage or app to determine which version performs better in measures like traffic or customer conversion rates. The basic design is to randomly assign your incoming traffic  to a website to one version (with an equal probability across all versions), collect data, and perform a two proportion test to determine whether performance between versions are statistically significantly different.

Nowadays a popular alternative to A/B testing known as Multi Armed Bandit experiments are getting a lot of attention in business practices. Here I will discuss the overview of the algorithm behind these experiment designs, discuss its benefits and short comings, and perform a simulation.


## Overview

<center>
<img src="http://neilpatel.com/wp-content/uploads/2016/06/image01-7.png" style="width: 500px;"/>
<p class="caption" style="font-style: italic; color: gray" > Figure: A/B Test  </p>
</center>

During an A/B test experiment, an equal number of samples are assigned to each version and thus the data collected is balanced. Theoretically speaking, this is the best way to collect data and perform hypothesis testing on if the ultimate goal is to find the ground truth on the difference between the two means or proportions. We will call this the "learning" phase (otherwise known as the "exploration" phase).

After some given period of time or number of samples collected, a hypothesis test is performed and at which point all future samples are then assigned to the top performing version. We will call this the "earning" phase (otherwise known as the "exploitation phase").

<center>
<img src="https://blog.automizy.com/wp-content/uploads/2017/03/AB-testing.png" style="width: 500px;"/>
<p class="caption" style="font-style: italic; color: gray" > Figure: A/B Test </p>
</center>

However where A/B test falls short is its inability to maximize the gain or minimize the cost of the experiment. In practice, deliberately assigning samples to a lower performing version may be very costly; for example in clinical trials where an experiment is performed to determine whether or not a drug is effective, if half of the samples are assigned to a placebo drug, those patients receiving a placebo drug may be subject to missing out on a treatment that may be life-saving. In business, assigning samples to a lower performing website may mean the company is missing out on business opportunity.

This is where a Multi Armed Bandit test may be more optimal: a Multi Armed Bandit is designed to optimize gain/minimize loss during the experiment, while simultaneously collectig data to perform hypothesis testing.

A Multi Armed Bandit experiment is designed with a mix of the two phases. During the exploration phase (which is typically shorter in a Multi Armed Bandit test than in an A/B test), samples are randomly and evenly assigned to a version, similar to an A/B test. However after some initial learning phase, samples are assigned proportionally larger to a higher performing version, either immediately or gradually, moving towards the earning phase rather quickly.

<center>
<img src="https://blog.automizy.com/wp-content/uploads/2017/03/Multi-armed-bandit.png" style="width: 500px;"/>
<p class="caption" style="font-style: italic; color: gray" >  Figure: Multi Armed Bandit </p>
</center>

 
## Experiment

Here we introduce some of the popular Multi Armed Bandit algorithms.

### A/B Test

```{r}
pacman::p_load(dplyr, ggplot2, data.table)

source("https://raw.githubusercontent.com/rjsaito/Just-R-Things/master/Machine%20Learning/Multi%20Armed%20Bandit/multi_armed_bandit_sim.R")

experiment_plot = function(x, y){
    
  x_sim = data.frame(ver = "x", x, stringsAsFactors = F); names(x_sim)[2] = "outcome"
  y_sim = data.frame(ver = "y", y, stringsAsFactors = F); names(y_sim)[2] = "outcome"

  xy_sim = rbind(x_sim, y_sim) %>%
    group_by(round, ver) %>%
    summarise(count = n()) %>%
    arrange(ver, round) %>%
    data.table()

  round_max = group_by(xy_sim, by = ver) %>% summarise(max_round = max(round))
  
  if(any(round_max$max_round < 20)){
    missing_round = suppressWarnings(apply(round_max, 1, function(z){
      max_round = as.numeric(z[2]) + 1
      ver = z[1]
      if((max_round-1) < 20) {
        data.frame(round = max_round:20, ver = ver, count = 0)
      }
   }) %>% do.call(rbind, .))
  }
  
  pl <- ggplot(data = xy_sim, aes(round, count, fill = ver)) + 
    geom_area(aes(fill = ver), position = 'stack')
  pl
}

ABtest_sim = ABtest(rounds= 20, n = 200, p.crit = 0, p = c(.4, .5))
experiment_plot(ABtest_sim$x, ABtest_sim$y)
```

#### Epsilon-Greedy Multi Armed Bandit Test

<center>
<img src="http://blog.thedataincubator.com/wp-content/uploads/2016/07/epsilongreedy-300x182.png" style="width: 400px;"/>
<p class="caption" style="font-style: italic; color: gray" >  Figure: Epsilon Greedy Multi Armed Bandit </p>
</center>



```{r}
eg_sim = mab_eg(rounds= 20, n = 200, p.crit = 0, p = c(.4, .5))
experiment_plot(eg_sim$x, eg_sim$y)
```


#### Epsilon-Decreasing Multi Armed Bandit Test


```{r}
ed_sim = mab_ed(rounds= 20, n = 200, p.crit = 0, p = c(.4, .5))
experiment_plot(ed_sim$x, ed_sim$y)
```


#### UCB1 Multi Armed Bandit Test

```{r}
ucb_sim = mab_ucb(rounds= 20, n = 200, p.crit = 0, p = c(.4, .5))
experiment_plot(ucb_sim$x, ucb_sim$y)
```


  UCB1

https://jeremykun.com/2013/10/28/optimism-in-the-face-of-uncertainty-the-ucb1-algorithm/


## Simulation

We have 4 potential experiment designs: 

* A/B Test
* Epsilon-Greedy Multi Armed Bandit Test
* Epsilon-Decreasing Multi Armed Bandit Test
* UCB1 Multi Armed Bandit Test

We ran a large number of replications for each simulation scenario (combination of test design, sample size, effect size, p-critical value of hypothesis tests). Simulated data are sampled from a binomial distribution of successes vs. failures.


Let us say that at each round we sample a total of 200 observations, split between group A and group B. 


<center>
<img src="https://github.com/rjsaito/Just-R-Things/blob/master/Machine%20Learning/Multi%20Armed%20Bandit/ab_mab_simulation_result_n_500.tiff" style="width: 700px;"/>
<p class="caption" style="font-style: italic; color: gray" >  Figure: Multi Armed Bandit </p>
</center>



